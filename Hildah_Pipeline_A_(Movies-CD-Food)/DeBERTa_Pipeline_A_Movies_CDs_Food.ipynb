{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNacbqY2218nd8bi7xi64b6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"747f7311486644f594eb5498123ac5ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f72b0e489fce46e9ade7fa430b97e7be","IPY_MODEL_55c7952ce0bb4dee89a2fa7017531f5a","IPY_MODEL_354de53b29a548d9852c2efcd93e8f50"],"layout":"IPY_MODEL_c88242cc4ef7404a962b7738a4735403"}},"f72b0e489fce46e9ade7fa430b97e7be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_239528ce22fe4a919c421d91f6370d58","placeholder":"​","style":"IPY_MODEL_0f54d90c62c04ea2b4a469ae27875d56","value":"Map: 100%"}},"55c7952ce0bb4dee89a2fa7017531f5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_430c3da5f7794cd3b296fa87356dc499","max":5000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18cd9a30cce84f58a843e51f85022e52","value":5000}},"354de53b29a548d9852c2efcd93e8f50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c36489a49064f4386b6b506135a25b5","placeholder":"​","style":"IPY_MODEL_908e369a0f6e4c18b4bb0ce57be6ae92","value":" 5000/5000 [00:00&lt;00:00, 9896.76 examples/s]"}},"c88242cc4ef7404a962b7738a4735403":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"239528ce22fe4a919c421d91f6370d58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f54d90c62c04ea2b4a469ae27875d56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"430c3da5f7794cd3b296fa87356dc499":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18cd9a30cce84f58a843e51f85022e52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c36489a49064f4386b6b506135a25b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"908e369a0f6e4c18b4bb0ce57be6ae92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"934f215c79c44b62b0c5ad5d1a616439":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_53b05a6c324b412a8b152a4970ab065d","IPY_MODEL_88c6bd6feeb749b9bef178a053807d96","IPY_MODEL_c0078514c4f045e5b34332e5ef66b358"],"layout":"IPY_MODEL_0bb3d5b8b3844316841c5d5f89296730"}},"53b05a6c324b412a8b152a4970ab065d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2bafa32665749cd95b1c124ebfa9d28","placeholder":"​","style":"IPY_MODEL_e2fd79f9528240baac7eb61330f16e4f","value":"Map: 100%"}},"88c6bd6feeb749b9bef178a053807d96":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_919f261821e14d50a33254c088fdfa59","max":5000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9aa57a3ed7854811a3db20ea2f2068b4","value":5000}},"c0078514c4f045e5b34332e5ef66b358":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06612a46854e4dd08b1e62e876e3f750","placeholder":"​","style":"IPY_MODEL_7c55188f19f24115b76f3f8aaa84a72b","value":" 5000/5000 [00:00&lt;00:00, 8910.82 examples/s]"}},"0bb3d5b8b3844316841c5d5f89296730":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2bafa32665749cd95b1c124ebfa9d28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2fd79f9528240baac7eb61330f16e4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"919f261821e14d50a33254c088fdfa59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9aa57a3ed7854811a3db20ea2f2068b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06612a46854e4dd08b1e62e876e3f750":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c55188f19f24115b76f3f8aaa84a72b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4febb438369e40ca8948cfe0560fb43c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_062b9ee432304b419b4dfdcfa7e86914","IPY_MODEL_f515fec0ccca484ab42ca207e1f4b809","IPY_MODEL_879215b72758486b8c25993de9ac8f50"],"layout":"IPY_MODEL_a2c49e22e622423bbae6d1b6168a2309"}},"062b9ee432304b419b4dfdcfa7e86914":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99781f291b66424c85689fdc440067f0","placeholder":"​","style":"IPY_MODEL_2e6eb0b76cc44b849966755fbd45ab40","value":"Map: 100%"}},"f515fec0ccca484ab42ca207e1f4b809":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6597a8697694be6aecb6a3c5a691ce7","max":5000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a294a0958b2143b99bc2c81e20e58251","value":5000}},"879215b72758486b8c25993de9ac8f50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7035c52d8b5544b7853750ec55b7d5a3","placeholder":"​","style":"IPY_MODEL_52e96bf179154177bbba94eb5c99dac5","value":" 5000/5000 [00:00&lt;00:00, 5986.35 examples/s]"}},"a2c49e22e622423bbae6d1b6168a2309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99781f291b66424c85689fdc440067f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e6eb0b76cc44b849966755fbd45ab40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6597a8697694be6aecb6a3c5a691ce7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a294a0958b2143b99bc2c81e20e58251":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7035c52d8b5544b7853750ec55b7d5a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52e96bf179154177bbba94eb5c99dac5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Exploring Transfer Learning Performance in NLP: A Cross-Dataset Generalization Study\n","\n","This experiment aims to answer the following research questions:\n","\n","\n","1.   Which BERT family model works best for transfer learning in NLP classification tasks?\n","2.   How does model performance vary when trained on related versus unrelated datasets?\n","3.   What are the optimal fine-tuning strategies for transfer learning?\n","4.   How much data is needed to achieve effective transfer learning effects,that is, 1,000, 5,000 records or 10,000 records?\n","5.   Does the sequence of data training in continuous learning matter?\n","\n","This jupyter notebook covers the first sequence(A) where the baseline is on the primary Amazon Reviews dataset on Movies and TV, followed by further training of the baseline model on the secondary data set CDs and Vinyl dataset from a similar domain i.e. entertainment, and finally this model is evaluated on the Grocery and Gourmet review dataset which is a different domain in the food industry."],"metadata":{"id":"FSswQ3m1iVp_"}},{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"ElUHIEaWjFsb"}},{"cell_type":"code","source":["!pip install datasets\n","!pip install evaluate\n","!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"DKmLBbwzibOK","executionInfo":{"status":"ok","timestamp":1744381768066,"user_tz":-180,"elapsed":11074,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}},"outputId":"c3eb8c68-447b-41dc-a0bd-46e1d2538019"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.3.2)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.3\n","Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.1)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, alembic, optuna\n","Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","execution_count":14,"metadata":{"id":"I2FtR35qiFlZ","executionInfo":{"status":"ok","timestamp":1744382122560,"user_tz":-180,"elapsed":13,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}}},"outputs":[],"source":["import pandas as pd\n","import optuna\n","import os\n","import datasets\n","from datasets import Dataset\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","import numpy as np\n","import evaluate\n","from sklearn.metrics import accuracy_score,precision_recall_fscore_support,f1_score\n","from sklearn.model_selection import ParameterGrid\n","from sklearn.utils import shuffle\n","from transformers import DataCollatorWithPadding\n","import torch\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQwoqJpRjRSp","executionInfo":{"status":"ok","timestamp":1744381811758,"user_tz":-180,"elapsed":24359,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}},"outputId":"253693d6-2f57-4fef-bcba-b41b2da36d60"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import json\n","\n","def jsonl_to_df(file_path):\n","    \"\"\"Loads a JSONL file into a Pandas DataFrame.\n","\n","    Args:\n","        file_path: The path to the JSONL file.\n","\n","    Returns:\n","        A Pandas DataFrame containing the data from the JSONL file, or None if an error occurs.\n","    \"\"\"\n","    try:\n","        data = []\n","        with open(file_path, 'r') as f:\n","            for line in f:\n","                try:\n","                    data.append(json.loads(line))\n","                except json.JSONDecodeError as e:\n","                    print(f\"Skipping invalid JSON line: {line.strip()}\")\n","                    print(f\"Error: {e}\")\n","        df = pd.DataFrame(data)\n","        return df\n","    except FileNotFoundError:\n","        print(f\"Error: File not found at {file_path}\")\n","        return None\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","        return None\n"],"metadata":{"id":"FP4BJNYKjTz6","executionInfo":{"status":"ok","timestamp":1744381811765,"user_tz":-180,"elapsed":5,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=1)\n","    # eval_metrics=metric.compute(predictions=predictions, references=labels)\n","    # f1 = f1_score(labels, predictions, average='weighted')\n","    accuracy = accuracy_score(labels, predictions)\n","    f1 = f1_score(labels, predictions, average='weighted')\n","    return {\"accuracy\": accuracy, \"f1\": f1}\n","    # return eval_metrics\n","\n","def prep_dataset(df):\n","    df = shuffle(df)\n","    df = df[df['rating'] != 3]\n","    df_subset = df[['text', 'rating']][:5000]\n","    df_subset['label'] = df['rating'].apply(lambda x: 1 if x > 4 else 0)\n","    df_subset = df_subset.drop('rating', axis=1)\n","    return df_subset\n"],"metadata":{"id":"zFJob1u8jXM2","executionInfo":{"status":"ok","timestamp":1744381811814,"user_tz":-180,"elapsed":30,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Movies and TV Dataset\n","# Load file from jsonl to df :\n","file_path = '/content/drive/MyDrive/ColabNotebooks/W266/final_project_a/shuffle_100k.jsonl'\n","df = jsonl_to_df(file_path)\n","\n","# Create binary classification\n","df_subset = prep_dataset(df)\n"],"metadata":{"id":"5M3hFUkHjaES","executionInfo":{"status":"ok","timestamp":1744381819107,"user_tz":-180,"elapsed":7286,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#  CDs and TV data set\n","# Load file from jsonl to df :\n","file_path = '/content/drive/MyDrive/ColabNotebooks/W266/final_project_a/shuffle_2_CDs_100k.jsonl'\n","df_cds = jsonl_to_df(file_path)\n","\n","# Create binary classification\n","df_cds_subset = prep_dataset(df_cds)\n"],"metadata":{"id":"JUURocVMjc7b","executionInfo":{"status":"ok","timestamp":1744381823542,"user_tz":-180,"elapsed":4432,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#  shuffle df_cds and create sample data set\n","# Load file from jsonl to df :\n","file_path = '/content/drive/MyDrive/ColabNotebooks/W266/final_project_a/shuffle_3_Food_100k.jsonl'\n","df_food = jsonl_to_df(file_path)\n","\n","df_food_subset = prep_dataset(df_food)"],"metadata":{"id":"VUA7VnQGjgAX","executionInfo":{"status":"ok","timestamp":1744381827044,"user_tz":-180,"elapsed":3500,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# microsoft/deberta-v3-base"],"metadata":{"id":"taoGR2Abqu0h"}},{"cell_type":"code","source":["BEST_MODEL_SAVE_PATH =\"drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_best\"\n","MODEL_PATH = \"drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_baseline\""],"metadata":{"id":"q96bJcVgrkNs","executionInfo":{"status":"ok","timestamp":1744381837611,"user_tz":-180,"elapsed":42,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Load tokenizer for Deberta\n","\n","\n","MAX_SEQUENCE_LENGTH = 50\n","\n","# Function to preprocess (tokenize) data\n","def tokenize_function(example):\n","    review_text = example['text']\n","    encoded = tokenizer.batch_encode_plus(\n","            review_text,\n","            max_length=MAX_SEQUENCE_LENGTH,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","    return encoded\n","\n","\n","# Apply Tokenization to each dataset\n","tokens_movies = Dataset.from_pandas(df_subset).map(tokenize_function, batched=True)\n","tokens_cds = Dataset.from_pandas(df_cds_subset).map(tokenize_function, batched=True)\n","tokens_food = Dataset.from_pandas(df_food_subset).map(tokenize_function, batched=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168,"referenced_widgets":["747f7311486644f594eb5498123ac5ba","f72b0e489fce46e9ade7fa430b97e7be","55c7952ce0bb4dee89a2fa7017531f5a","354de53b29a548d9852c2efcd93e8f50","c88242cc4ef7404a962b7738a4735403","239528ce22fe4a919c421d91f6370d58","0f54d90c62c04ea2b4a469ae27875d56","430c3da5f7794cd3b296fa87356dc499","18cd9a30cce84f58a843e51f85022e52","6c36489a49064f4386b6b506135a25b5","908e369a0f6e4c18b4bb0ce57be6ae92","934f215c79c44b62b0c5ad5d1a616439","53b05a6c324b412a8b152a4970ab065d","88c6bd6feeb749b9bef178a053807d96","c0078514c4f045e5b34332e5ef66b358","0bb3d5b8b3844316841c5d5f89296730","b2bafa32665749cd95b1c124ebfa9d28","e2fd79f9528240baac7eb61330f16e4f","919f261821e14d50a33254c088fdfa59","9aa57a3ed7854811a3db20ea2f2068b4","06612a46854e4dd08b1e62e876e3f750","7c55188f19f24115b76f3f8aaa84a72b","4febb438369e40ca8948cfe0560fb43c","062b9ee432304b419b4dfdcfa7e86914","f515fec0ccca484ab42ca207e1f4b809","879215b72758486b8c25993de9ac8f50","a2c49e22e622423bbae6d1b6168a2309","99781f291b66424c85689fdc440067f0","2e6eb0b76cc44b849966755fbd45ab40","b6597a8697694be6aecb6a3c5a691ce7","a294a0958b2143b99bc2c81e20e58251","7035c52d8b5544b7853750ec55b7d5a3","52e96bf179154177bbba94eb5c99dac5"]},"id":"jmSWxGfKxYLm","executionInfo":{"status":"ok","timestamp":1744382134733,"user_tz":-180,"elapsed":3212,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}},"outputId":"a8ca56ed-41ff-4d97-e3c4-9fb868934b80"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"747f7311486644f594eb5498123ac5ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934f215c79c44b62b0c5ad5d1a616439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4febb438369e40ca8948cfe0560fb43c"}},"metadata":{}}]},{"cell_type":"code","source":["# Baseline\n","model_name = \"microsoft/deberta-v3-base\"\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","split_datasets = tokens_movies.train_test_split(test_size=0.2)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\"\n",")\n","\n","\n","# Create Trainer instance\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=split_datasets[\"train\"],\n","    eval_dataset=split_datasets[\"test\"],\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","trainer.evaluate()\n","\n","# Save the trained model\n","trainer.save_model(MODEL_PATH)\n","tokenizer.save_pretrained(MODEL_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":678},"id":"zNhSQoUuj5hz","executionInfo":{"status":"ok","timestamp":1744382423305,"user_tz":-180,"elapsed":121299,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}},"outputId":"c109809c-04de-4b55-be27-b13e2355ff88"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-18-5b679d22a5d7>:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhngondoki\u001b[0m (\u001b[33mhngondoki-uc-berkeley\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.9"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250411_143843-1vu3buqr</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/hngondoki-uc-berkeley/huggingface/runs/1vu3buqr' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/hngondoki-uc-berkeley/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/hngondoki-uc-berkeley/huggingface' target=\"_blank\">https://wandb.ai/hngondoki-uc-berkeley/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/hngondoki-uc-berkeley/huggingface/runs/1vu3buqr' target=\"_blank\">https://wandb.ai/hngondoki-uc-berkeley/huggingface/runs/1vu3buqr</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [750/750 01:31, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.448313</td>\n","      <td>0.809000</td>\n","      <td>0.813202</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.388700</td>\n","      <td>0.461501</td>\n","      <td>0.817000</td>\n","      <td>0.814726</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.388700</td>\n","      <td>0.610474</td>\n","      <td>0.813000</td>\n","      <td>0.809783</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["('drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_baseline/tokenizer_config.json',\n"," 'drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_baseline/special_tokens_map.json',\n"," 'drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_baseline/spm.model',\n"," 'drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_baseline/added_tokens.json',\n"," 'drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_baseline/tokenizer.json')"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## Hyperparameter tuning"],"metadata":{"id":"_BXRWr5GrzkT"}},{"cell_type":"code","source":["#Fine tuning\n","best_accuracy = 0.0\n","best_trainer = None\n","\n","def objective(trial):\n","    global best_accuracy, best_trainer\n","\n","    # Load the saved model\n","    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=2)\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","\n","    # Define hyperparameters to be optimized\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n","    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16])\n","    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 3)\n","    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n","\n","    # Define training arguments\n","    training_args = TrainingArguments(\n","        output_dir=BEST_MODEL_SAVE_PATH,\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=num_train_epochs,\n","        weight_decay=weight_decay,\n","        evaluation_strategy=\"epoch\",\n","    )\n","\n","    # Create Trainer instance\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=split_datasets[\"train\"],\n","        eval_dataset=split_datasets[\"test\"],\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n","    )\n","\n","    # Train the model\n","    trainer.train()\n","\n","    result = trainer.evaluate()\n","    accuracy = result[\"eval_accuracy\"]\n","\n","    # Save the best model\n","    if accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        best_trainer = trainer\n","\n","    return accuracy\n","\n","# Create Optuna study\n","study = optuna.create_study(direction=\"maximize\")  # Maximize the evaluation metric\n","study.optimize(objective, n_trials=10)  # Run 10 trials\n","\n","if best_trainer:\n","    best_trainer.save_model(BEST_MODEL_SAVE_PATH)\n","    tokenizer.save_pretrained(BEST_MODEL_SAVE_PATH)\n","    print(f\"Best model saved to: {BEST_MODEL_SAVE_PATH}\")\n","\n","# Print best hyperparameters\n","print(\"Best trial:\")\n","trial = study.best_trial\n","print(f\"  Value: {trial.value}\")\n","print(f\"  Params: \")\n","for key, value in trial.params.items():\n","    print(f\"    {key}: {value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KCQkApheZ9nM","executionInfo":{"status":"ok","timestamp":1744385333913,"user_tz":-180,"elapsed":941685,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}},"outputId":"c7e1bc93-0da9-4d6f-fa1d-5840259b6aaf"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:13:12,005] A new study created in memory with name: no-name-ffbadcac-18ac-41b2-9f24-d21785336714\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/1500 03:16, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.254100</td>\n","      <td>0.935246</td>\n","      <td>0.800000</td>\n","      <td>0.801333</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.188600</td>\n","      <td>1.025829</td>\n","      <td>0.807000</td>\n","      <td>0.800618</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.117500</td>\n","      <td>1.108276</td>\n","      <td>0.807000</td>\n","      <td>0.803442</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:16:31,688] Trial 0 finished with value: 0.807 and parameters: {'learning_rate': 2.4523406398393977e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.013373529213940876}. Best is trial 0 with value: 0.807.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [250/250 00:41, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.767150</td>\n","      <td>0.809000</td>\n","      <td>0.807066</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:17:16,303] Trial 1 finished with value: 0.809 and parameters: {'learning_rate': 2.183833603251549e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 1, 'weight_decay': 0.09969889902292885}. Best is trial 1 with value: 0.809.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 00:58, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.220000</td>\n","      <td>0.862532</td>\n","      <td>0.810000</td>\n","      <td>0.805155</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:18:17,728] Trial 2 finished with value: 0.81 and parameters: {'learning_rate': 1.3713298378307126e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'weight_decay': 0.03772717506228008}. Best is trial 2 with value: 0.81.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [250/250 00:36, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.767056</td>\n","      <td>0.809000</td>\n","      <td>0.807066</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:18:56,936] Trial 3 finished with value: 0.809 and parameters: {'learning_rate': 2.1893686358369264e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 1, 'weight_decay': 0.06771895994106733}. Best is trial 2 with value: 0.81.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1000/1000 01:51, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.224200</td>\n","      <td>0.976736</td>\n","      <td>0.796000</td>\n","      <td>0.798423</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.161700</td>\n","      <td>1.002884</td>\n","      <td>0.810000</td>\n","      <td>0.807078</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:20:51,037] Trial 4 finished with value: 0.81 and parameters: {'learning_rate': 1.4409173580489472e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.04070841489737849}. Best is trial 2 with value: 0.81.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 01:08, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.217500</td>\n","      <td>0.858941</td>\n","      <td>0.813000</td>\n","      <td>0.807855</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:22:02,304] Trial 5 finished with value: 0.813 and parameters: {'learning_rate': 1.1724474749197321e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'weight_decay': 0.02601230279948622}. Best is trial 5 with value: 0.813.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/1500 02:48, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.237600</td>\n","      <td>0.971110</td>\n","      <td>0.795000</td>\n","      <td>0.796273</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.171000</td>\n","      <td>1.037882</td>\n","      <td>0.808000</td>\n","      <td>0.800096</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.121400</td>\n","      <td>1.139859</td>\n","      <td>0.805000</td>\n","      <td>0.802118</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:24:53,847] Trial 6 finished with value: 0.805 and parameters: {'learning_rate': 1.7426223766805716e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.04724444938567892}. Best is trial 5 with value: 0.813.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [750/750 01:50, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.835387</td>\n","      <td>0.773000</td>\n","      <td>0.775216</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.148000</td>\n","      <td>1.150187</td>\n","      <td>0.810000</td>\n","      <td>0.798779</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.148000</td>\n","      <td>1.199747</td>\n","      <td>0.802000</td>\n","      <td>0.800763</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:26:47,035] Trial 7 finished with value: 0.802 and parameters: {'learning_rate': 4.1409725156924684e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3, 'weight_decay': 0.06477005632621119}. Best is trial 5 with value: 0.813.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [250/250 00:40, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.806939</td>\n","      <td>0.796000</td>\n","      <td>0.795376</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:27:42,933] Trial 8 finished with value: 0.796 and parameters: {'learning_rate': 3.561529435952857e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 1, 'weight_decay': 0.05714505306153455}. Best is trial 5 with value: 0.813.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-23-911076b9e3bc>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 01:04, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.956945</td>\n","      <td>0.783000</td>\n","      <td>0.787171</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.124300</td>\n","      <td>1.008653</td>\n","      <td>0.798000</td>\n","      <td>0.798200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-04-11 15:28:49,818] Trial 9 finished with value: 0.798 and parameters: {'learning_rate': 1.6153438949435315e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2, 'weight_decay': 0.017411101631749935}. Best is trial 5 with value: 0.813.\n"]},{"output_type":"stream","name":"stdout","text":["Best model saved to: drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_best\n","Best trial:\n","  Value: 0.813\n","  Params: \n","    learning_rate: 1.1724474749197321e-05\n","    per_device_train_batch_size: 8\n","    num_train_epochs: 1\n","    weight_decay: 0.02601230279948622\n"]}]},{"cell_type":"markdown","source":["## Train on CD and Vinyl Dataset\n"],"metadata":{"id":"7HpwSg3gdK2d"}},{"cell_type":"code","source":["# Load best model\n","model = AutoModelForSequenceClassification.from_pretrained(BEST_MODEL_SAVE_PATH)\n","tokenizer = AutoTokenizer.from_pretrained(BEST_MODEL_SAVE_PATH)\n","TRAINING_ARGS=f\"{BEST_MODEL_SAVE_PATH}/training_args.bin\"\n","CD_MODEL_PATH = \"drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_CD\"\n","\n","# Split the data\n","split_datasets = tokens_cds.train_test_split(test_size=0.2)\n","\n","training_args = torch.load(TRAINING_ARGS, weights_only=False)\n","\n","# Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=split_datasets[\"train\"],\n","    eval_dataset=split_datasets[\"test\"],\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train\n","trainer.train()\n","\n","# save model\n","\n","model.save_pretrained(CD_MODEL_PATH)\n","tokenizer.save_pretrained(CD_MODEL_PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"id":"mL4vfXKRdPWW","executionInfo":{"status":"ok","timestamp":1744385841941,"user_tz":-180,"elapsed":60844,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}},"outputId":"8deae68d-cfb1-4912-e31c-5964b8a29b0e"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-27-5538e7512dcf>:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 00:56, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.417700</td>\n","      <td>0.468190</td>\n","      <td>0.807000</td>\n","      <td>0.784293</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["('drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_CD/tokenizer_config.json',\n"," 'drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_CD/special_tokens_map.json',\n"," 'drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_CD/spm.model',\n"," 'drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_CD/added_tokens.json',\n"," 'drive/MyDrive/ColabNotebooks/W266/final_project_a/deberta_Movies_CD/tokenizer.json')"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["## Evaluate on Food"],"metadata":{"id":"OJ9Qv531fSfE"}},{"cell_type":"code","source":["\n","model = AutoModelForSequenceClassification.from_pretrained(CD_MODEL_PATH)\n","tokenizer = AutoTokenizer.from_pretrained(CD_MODEL_PATH)\n","\n","# Train\n","trainer = Trainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","results = trainer.evaluate(eval_dataset=tokens_food)\n","print(\"\\nEvaluation Results:\", results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"id":"WqNi2WX7fY14","executionInfo":{"status":"ok","timestamp":1744385910787,"user_tz":-180,"elapsed":10939,"user":{"displayName":"Hildah N","userId":"17881383761401172763"}},"outputId":"fe6ec894-ef50-4edd-f20a-494e3d7b7f29"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-29-a87ac7cfe94c>:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [625/625 00:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Evaluation Results: {'eval_loss': 0.3490177392959595, 'eval_model_preparation_time': 0.0019, 'eval_accuracy': 0.8724, 'eval_f1': 0.8701505873398575, 'eval_runtime': 10.0456, 'eval_samples_per_second': 497.73, 'eval_steps_per_second': 62.216}\n"]}]}]}