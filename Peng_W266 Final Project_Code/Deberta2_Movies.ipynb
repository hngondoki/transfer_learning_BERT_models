{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNMAMGfDJC4nRzgVyUYKclj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Step 0: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7KcAcrV0pTw8","executionInfo":{"status":"ok","timestamp":1743700051150,"user_tz":420,"elapsed":23114,"user":{"displayName":"Vincent Zhao","userId":"15403960931610520032"}},"outputId":"77956eec-f014-451d-bb54-67b56faead0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Step 1: Install dependencies and import libraries\n","!pip install -q transformers\n","!pip install -q torchinfo\n","!pip install -q datasets\n","!pip install -q evaluate\n","!pip install -q optuna\n","!pip install -q wandb\n","\n","import wandb\n","# Log in to wandb. Replace \"your_api_key_here\" with your actual WANDB API key.\n","wandb.login(key=\"your_api_key_here\")\n","\n","from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer,\n","                          TrainingArguments, EarlyStoppingCallback)\n","from datasets import load_dataset\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import os\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6G2bpQxKpTt3","executionInfo":{"status":"ok","timestamp":1743700090047,"user_tz":420,"elapsed":38896,"user":{"displayName":"Vincent Zhao","userId":"15403960931610520032"}},"outputId":"5b645029-6e98-4c1d-8f47-852f893b9d50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeng_zhao\u001b[0m (\u001b[33mpeng_zhao-university-of-california-berkeley\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}]},{"cell_type":"code","source":["# Step 2: Load and preprocess the new dataset (Movies_and_TV)\n","dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Movies_and_TV\", trust_remote_code=True)\n","\n","# Shuffle the dataset and select a subset\n","shuffled_dataset = dataset[\"full\"].shuffle(seed=42)\n","subset_size = int(0.2 * len(shuffled_dataset))  # use 20% of the data\n","subset_dataset = shuffled_dataset.select(range(subset_size))\n","\n","# Filter out samples where the rating equals 3\n","subset_dataset = subset_dataset.filter(lambda x: x[\"rating\"] != 3)\n","\n","# Initialize the DeBERTa tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n","\n","# Define the tokenization and formatting function\n","def tokenize_and_format(examples):\n","    outputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n","    outputs[\"labels\"] = [1 if rating > 3 else 0 for rating in examples[\"rating\"]]\n","    return outputs\n","\n","# Tokenize the new dataset\n","tokenized_dataset = subset_dataset.map(tokenize_and_format, batched=True)\n","\n","# Save a checkpoint of the tokenized dataset\n","tokenized_dataset.save_to_disk(\"/content/drive/MyDrive/FP/Checkpoints/tokenized_movies_checkpoint\")\n","\n","# Split into train/test and set to PyTorch tensors\n","tokenized_datasets = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n","tokenized_datasets.set_format(\"torch\")\n"],"metadata":{"id":"3RN-bQ_0pTqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 2': Load from checkpoint if needed\n","from datasets import load_from_disk\n","tokenized_dataset = load_from_disk(\"/content/drive/MyDrive/FP/Checkpoints/tokenized_movies_checkpoint\")\n","tokenized_datasets = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n","tokenized_datasets.set_format(\"torch\")\n"],"metadata":{"id":"fcO1kWKqpTnv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Load the previously fine-tuned DeBERTa model from the CDs training\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    \"/content/drive/MyDrive/FP/Checkpoints/final_checkpoint_CDs_deberta\", num_labels=2\n",")\n","\n","# Define the compute_metrics function for evaluation\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n","    acc = accuracy_score(labels, predictions)\n","    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"],"metadata":{"id":"_uzEoTvnpTlJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Set up TrainingArguments for continued fine-tuning\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/FP/Movies_results_continued\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=128,\n","    per_device_eval_batch_size=64,\n","    gradient_accumulation_steps=1,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    fp16=True,\n","    dataloader_num_workers=6,\n","    warmup_steps=500,\n","    report_to=[\"wandb\"],\n","    run_name=\"deberta_movies_and_tv_finetune\",\n","    logging_steps=50,\n","    logging_first_step=True\n",")\n","\n","# Print number of training and evaluation samples\n","total_train = len(tokenized_datasets[\"train\"])\n","total_eval = len(tokenized_datasets[\"test\"])\n","print(\"Total training samples:\", total_train)\n","print(\"Total evaluation samples:\", total_eval)\n","\n","# Use partial data for actual training and eval\n","train_subset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(int(0.05 * total_train)))\n","eval_subset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(int(0.05 * total_eval)))\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_subset,\n","    eval_dataset=eval_subset,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hw8d5XgIpTin","executionInfo":{"status":"ok","timestamp":1743700134019,"user_tz":420,"elapsed":10429,"user":{"displayName":"Vincent Zhao","userId":"15403960931610520032"}},"outputId":"e715911b-0f85-4a5c-99b7-e5d87109cfd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Total training samples: 2571408\n","Total evaluation samples: 642852\n"]}]},{"cell_type":"code","source":["# Step 5: Continue fine-tuning\n","train_result = trainer.train()\n","\n","# Print checkpoint and metrics\n","print(\"Best model checkpoint:\", trainer.state.best_model_checkpoint)\n","print(\"Best validation metric:\", trainer.state.best_metric)\n","\n","eval_results = trainer.evaluate()\n","print(\"Evaluation results of the best model:\", eval_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"WB1k1ON3pTfq","outputId":"59e25435-bd52-426a-8c18-438abd4ebd62","executionInfo":{"status":"ok","timestamp":1743704207529,"user_tz":420,"elapsed":4073151,"user":{"displayName":"Vincent Zhao","userId":"15403960931610520032"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.8"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250403_170855-exvrlkhe</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/peng_zhao-university-of-california-berkeley/huggingface/runs/exvrlkhe' target=\"_blank\">deberta_movies_and_tv_finetune</a></strong> to <a href='https://wandb.ai/peng_zhao-university-of-california-berkeley/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/peng_zhao-university-of-california-berkeley/huggingface' target=\"_blank\">https://wandb.ai/peng_zhao-university-of-california-berkeley/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/peng_zhao-university-of-california-berkeley/huggingface/runs/exvrlkhe' target=\"_blank\">https://wandb.ai/peng_zhao-university-of-california-berkeley/huggingface/runs/exvrlkhe</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4020' max='5025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4020/5025 1:06:31 < 16:38, 1.01 it/s, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.095100</td>\n","      <td>0.084693</td>\n","      <td>0.968981</td>\n","      <td>0.968496</td>\n","      <td>0.968397</td>\n","      <td>0.968981</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.065600</td>\n","      <td>0.078376</td>\n","      <td>0.972279</td>\n","      <td>0.971973</td>\n","      <td>0.971863</td>\n","      <td>0.972279</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.044100</td>\n","      <td>0.089033</td>\n","      <td>0.972435</td>\n","      <td>0.972432</td>\n","      <td>0.972430</td>\n","      <td>0.972435</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.032300</td>\n","      <td>0.101287</td>\n","      <td>0.971657</td>\n","      <td>0.971783</td>\n","      <td>0.971943</td>\n","      <td>0.971657</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Could not locate the best model at /content/drive/MyDrive/FP/Movies_results_continued/checkpoint-2010/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"]},{"output_type":"stream","name":"stdout","text":["Best model checkpoint: /content/drive/MyDrive/FP/Movies_results_continued/checkpoint-2010\n","Best validation metric: 0.07837598025798798\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='503' max='503' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [503/503 01:16]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation results of the best model: {'eval_loss': 0.10128685086965561, 'eval_accuracy': 0.9716570219650302, 'eval_f1': 0.9717833744396753, 'eval_precision': 0.9719426493322775, 'eval_recall': 0.9716570219650302, 'eval_runtime': 76.6332, 'eval_samples_per_second': 419.426, 'eval_steps_per_second': 6.564, 'epoch': 4.0}\n"]}]},{"cell_type":"code","source":["# Save the final model\n","trainer.save_model(\"/content/drive/MyDrive/FP/Checkpoints/final_checkpoint_movies_deberta\")"],"metadata":{"id":"LPyy7CA_pTcm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QG0A37C_pTZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mJTbvJVYYuAy"},"execution_count":null,"outputs":[]}]}